# QA_RAG_PIPELINE_USING_LANGCHAIN

![image](https://github.com/user-attachments/assets/b20b6cc6-9cb4-46ab-ae35-caab84f315ae)
Limitation of LLM is context length . and the chunks later are converted in vectors and stored in vector databases.

![image](https://github.com/user-attachments/assets/d12960ba-e61a-42e2-a975-27812137e56c)

Here we give a question to the prompt an the data related to the question is retreived from the databases and given to the LLM model to answer our queries
